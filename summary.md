這段影片主要探討了Meta提出的「多Token預測」（Multi-Token Prediction, MTP）的概念，以及DeepSeek提出的「Token順序預測」（Token Order Prediction, TOP）作為MTP的輔助目標。

**多Token預測 (MTP):**

*   **概念：** 傳統的語言模型一次預測一個token，而MTP則一次預測多個token。
*   **實驗：** Meta的實驗顯示，130億參數的模型在預測4個token時，在HumanEval和MBPP上表現優於只預測下一個token的基準模型。
*   **優點：** 在需要「向前看」的任務（如數學計算、程式碼）表現較好，可以加快預測速度。
*   **缺點：**
    *   在標準NLP任務（如語意分析）上準確度下降。
    *   難以決定最佳的輸出head數量（即預測多少個token），因為這取決於模型的用途。
    *   較小的模型（小於10億參數）使用MTP效果更差。
    *   通用性不佳，訓練較困難。

**DeepSeek以MTP作為輔助目標：**

*   **方法：** 在訓練期間使用MTP作為輔助目標，訓練完成後移除。
*   **問題：** 準確預測多個token非常困難，會使學習信號變得嘈雜。

**Token順序預測 (TOP):**

*   **概念：** 作為MTP的替代方案，TOP讓模型在預測下一個token的同時，學習所有可能token的相對順序。模型預測哪些字詞最有可能在不久的將來出現，建立一個排序列表。
*   **方法：** 將排序列表預測作為額外的預測head，並設定一個固定窗口，限制模型不會看得太遠。如果一個字詞從未出現在窗口中，則標記為「永遠不會出現」。
*   **優點：**
    *   可以更好地捕捉語法和句法結構，因為有些模式間隔較遠，傳統的下一個token預測難以捕捉。
    *   作為輔助目標，訓練後可以移除head，模型仍可正常運行。
    *   TOP是比MTP更柔和的目標，更容易學習，因為預測token的順序比猜測確切的下一個token更容易。
*   **實驗結果：** 在多個自然語言處理基準測試中，TOP在大多數任務中勝過NTP和MTP。
*   **總結：** TOP是一種更有潛力的輔助目標，因為它的學習信號不那麼嘈雜，但仍需要更多的研究和比較。

**贊助商 Savala from Kinsta：**

*   影片中穿插了 Savala 的產品介紹，Savala是一個應用程式部署和託管平台，強調簡單易用、可擴展性強，且提供免費的內部流量、無限的協作者和並行構建，以及無限的資料庫使用量。它允許開發者將應用程式、儲存空間和資料庫放在同一個地方，並提供企業級的安全性。

**總結：**

影片介紹了MTP和TOP這兩種方法，旨在改善語言模型的性能。MTP直接預測多個token，而TOP則預測token的相對順序。TOP被認為是MTP更柔和、更易於學習的替代方案，並在實驗中顯示出一定的優勢。影片最後也提到雖然TOP很有潛力，但仍需要更多的研究來驗證其有效性。
